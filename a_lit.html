<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Literature</title>
  <!-- icon -->
  <link rel="icon" href="./img/attent.ico" />
  <!-- css -->
  <link rel="stylesheet" type="text/css" href="css/style.css" />
  <!-- Webfont -->
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
  <!-- Font Awesome -->
  <link href="./fontawesome/css/all.css" rel="stylesheet">
</head>

<body>

  <header id="top">
    <nav>
      <a href="index.html">The Attention System (ATS)</a>
      <a href="a_alr.html">Alerting</a>
      <a href="a_ort.html">Orienting</a>
      <a href="a_exe.html">Executive</a>
      <a href="a_ext.html">Extending the ATS-Framework</a>
      <a href="#">Literature</a>
      <a href="a_wks.html" target="_blank">Attention Training Workshop</a>
    </nav>
    <h1><i class="fa fa-brain"></i> Literature</h1>
  </header>
  <main>
    <pre>
      The market opened quietly on Wall Street on April 23, 2013. As traders sipped lattes on an unseasonably chilly morning, stocks made modest gains from the opening bell to the lunch hour. But as everyone broke for lunch, the Associated Press broke a story on Twit- ter that changed the market's mood. Phones chirped at eateries in New York, Washington, and the world over as the news was retweeted again and again, creating an information cascade that swept through the Hype Machine in seconds. The tweet, which appeared at 1:07 p.m. eastern time in the United States, simply read “Breaking: Two Explosions in the White House and Barack Obama is injured.” It was retweeted over four thousand times in five minutes, which would have informed hundreds of thousands if not millions of people of the attack on the White House.
      
      You could almost hear the iced teas and Arnold Palmers being snorted back into the glasses of those monitoring the social network. The news was shocking. Putting aside fence jumpers, who are usually tackled on sight, there have only ever been four breaches of White House security that made it to the building. So two explosions injuring the president inside the White House was big news. The market stuttered. Then it skipped a beat. If only individual retail investors had been influenced by the news, the financial impact might have been contained. But the Hype Machine doesn't exist in isolation.
      
      It’s coupled to systems that sense, mine, analyze, and trade on sentiment expressed on social media in real time. Dataminr, RavenPack,and other companies are constantly sifting through social media datato find the signal in the noise. When they find that signal, they seize onit and relay instructions to their institutional clients to buy or sell aheadof market trends. On this particular afternoon, the sentiment wasn'tgood, and the data miners issued sell recommendations that triggeredautomated-trading algorithms to unload their stocks. When they did,the Dow fell instantly by nearly 200 points, wiping out $139 billion inequity value in seconds.
      
      But the news wasn't true. The White House was calm, and the president was fine. The tweet was fake news propagated by Syrian hackerswho had infiltrated the AP’s Twitter handle. There was a terrorist attack that day—just not at 1600 Pennsylvania Avenue. The attack happened on Twitter, and the casualties were felt on Wall Street. Themarket rebounded, but real people lost real money as their buy and sellorders were honored. Those who were late to the fire sale lost theirshirts. The “Hack Crash” of 2013 highlights the fragility of the sociotechnical systems we've wired into the Hype Machine. When newscascades through the network, it’s hard to stop and harder to verifywith enough time to prevent panic. When the news is false, it canwreak havoc on financial systems, health systems, and democratic institutions, creating real consequences from virtual falsity.
      
      
      Here's another example. When Hurricane Harvey hit southern Texas in the summer of 2017, the flooding displaced thousands and halted production at several oil refineries in the southern United States. News of gas shortages spread quickly on Twitter and Facebook as drivers posted pictures of long lines at gas stations, with makeshift signs say ing they were out of gas. A panic ensued, and drivers in the region rushed to stockpile gas as if the world were ending, creating a run on fuel in Austin, Dallas, Houston, and San Antonio.
      
      But as the authorities later revealed, there was no gas shortage. It was false news spread through social media, then picked up and re ported by broadcast media. As we later learned, there was plenty of gas to go around. The refinery and highway closures only slowed deliver ies. Had everyone stuck to their normal consumption, the distribution system could have handled the disruption, and there would have been  o shortage. The panic and the subsequent run on gas, however, en sured there was a shortage, wrought by manic stockpiling, driven by social media.
      
      The signature of a fake news crisis was repeating itself: false infor mation was spreading faster than the truth, misdirecting real behav iors with real impact. Such fake news can have dramatic consequences for businesses, democracies, and public health. And although it has been around for centuries, the speed and scale with which it spreads through the Hype Machine creates a fake news crisis on steroids.
      
      These vignettes highlight a systematic pattern in the spread of fake news (one that bears out in large-scale studies of the phenomenon that I'll discuss shortly). When fake news isn't completely fabricated, it typ ically distorts real-world information by tweaking or contorting it, mixing it with true information, and highlighting its most sensational and emotional elements. It then scales rapidly on social media and spreads faster than our ability to verify or debunk it. Once it spreads, it’s hard to put back in the bottle and even harder to clean up, even with a healthy dose of the truth.
      
      The Syrian hack that crashed the stock market in 2013 is a case study in the economic consequences of fake news. You've probably heard similar stories. The fact-checking site Snopes keeps a list of “hot 50” rumors that gets updated with alarming regularity. There was a 2008 rumor that United Airlines was filing for bankruptcy, a 2017 re port that Starbucks would give out free Frappuccinos to undocu mented workers, and President Trump's tweets, in March 2018, falsely claiming that Amazon was evading taxes, which sent shares of the company plummeting to their worst monthly performance in two years. But is there a systematic effect of fake news on businesses? On stock prices? Before we can understand the full implications of fake news, we need to detour through the story of a D-list actress named Kamilla Bjorlin.
      
      An Actress’s Rise to Fake News Infamy.
      Kamilla Bjorlin had been an actress her whole life. Since age seven, she had played bit parts in movies like Raising Helen, with Kate Hud son, and The Princess Diaries 2, with Anne Hathaway. She never really became a star, but narrative fiction was in her blood. So in 2011 she struck out on a different career path, forming a public relations and social media firm called Lidirigo Holdings, specializing in investor re lations and promotional research for public companies, including many biopharmaceutical firms. In fact, as alleged in a 2014 SEC complaint, Lidingo was a fake news factory engaged in “pump and dump” stock promotion schemes that posted fake articles to crowdsourced investor intelligence websites like Seeking Alpha, The Street, Yahoo! Finance, Forbes, and Investing.com, for the express purpose of moving stock prices.
      
      Writers employed by Lidingo created pseudonyms, like the Swiss Trader, Amy Baldwin, and the Trading Maven; claimed to have MBAs and degrees in physics; and wrote fake stories praising the growth or stability of the companies they were paid to promote. But they never disclosed their financial relationship with their clients, which was why they ran afoul of the SEC. Between 2011 and 2014, Lidingo allegedly published over four hundred fake news articles and earned cash and equity payments of more than $1 million. As part of its crackdown on fake financial news, a separate SEC complaint against a similar firm called DreamTeam described an operation that provided “social media relations, marketing and branding services” to publicly traded compa nies and “leveraged its extensive online social network to maximize exposure” to fake news designed to inflate its clients” stock prices.
      
      One company that Lidingo wrote fake news for was Galena Bio pharma. In the summer of 2013, before Lidingo was hired, Galena’s stock price was hovering around $2 per share. Between August 2013 and February 2014, Lidingo published twelve fake news stories about Galena, claiming, for example, that the company would make “a good long-term growth investment” because it had “a promising pipeline for revenue growth” from “three strong drugs in its pipeline”. 
      
      After the first two fake news articles were published, the com pany issued 17.5 million new shares in a secondary equity offering (SEQ) worth $32.6 million to the firm. Then five more fake news ar ticles were published, and the stock price skyrocketed. At a Novem ber 22 board meeting, the company granted hundreds of thousands of new stock options to the CEQ, COQ, CMO, and each of six directors. The stock price continued to rise. In fact, it rose over 925 percent be tween August 2013 and January 2014 (Figure 2.1). At a January  board meeting, then-CEO Mark Ahn declared insiders could trade the company’s stock immediately, and starting the next day, they did, un- loading $16 million worth in four weeks. 
      
      We all know that information moves markets, but the financial impact of fake news is not immediately clear. The Galena Biopharma story directly ties the two together. But of course, Galena is a one-off. Is there any systematic, generalizable evidence of the financial impact of fake news? Luckily Galena is one of over seven thousand firms that Shimon Kogan, Tobias Moskowitz, and Marina Niessner analyzed in their large-scale study of the relationship between fake news and financial markets.
      
      
      Fake News and Financial Markets Kogan.	
      
      Moskowitz, and Niessner examined data from the SEC’s udercover investigation, which identified fake news articles about pub- lic companies written to manipulate stock prices. By analyzing these validated fake news stories, Kogan, Moskowitz, and Niessner were able to systematically link the dissemination of fake news to stock price movements over time. The initial data covered a small number of articles on a small set of companies involved in the SEC investiga tion—171 articles on 47 companies. But the researchers broadened their sample by identifying fake news using a linguistic analysis of all the articles published on Seeking Alpha from 2005 to 2015, and on Motley Fool from 2009 to 2014. The larger sample, which looked for linguistic signatures of deceptive writing, was noisier than the verified SEC sample, but it allowed the authors to examine over 350,000 arti cles on over 7,500 companies over ten years. They also analyzed inves tors reactions to fake news before and after the SEC publicly announced its sting operation, which drew attention to the prevalence of fake news on these sites. The findings reveal a great deal about how fake news moves markets.
      
      In the verified SEC data, the publication of fake news was strongly correlated with increased trading volume. Abnormal trading volume rose by 37 percent over the three days following the publication of real news articles and 50 percent more following the publication of fake news articles relative to real news articles. In other words, investors reacted to fake news even more strongly than to real news. The reac tions were more pronounced for smaller firms and for firms with a greater percentage of retail (as opposed to institutional) investors. Fake articles were clicked on and read more often than real articles, and trading volume increased with the number of clicks and times an article was read.
      
      And the effect of fake news on stock prices? Fake articles had, on average, nearly three times the impact of real news articles on the daily price volatility or absolute return of the manipulated stocks in the three days after the publication of fake news, even after controlling for recent SEC filings, firm press releases, and return volatility in the days leading up to the article.
      
      The SEC went public with its investigation of fake news in 2014, fil ing lawsuits against several companies and the fake news factories they worked with, including Lidingo and DreamTeam. When the public revelation of the sting operation drew investors’ attention to the fact that fake news was being published on websites like Seeking Alpha, Kogan et al. used the public revelation of the sting to examine whether increased awareness of fake news eroded consumers’ trust in real news. Not surprisingly, real news had a bigger impact on trading volume and price volatility before the SEC announcement than after it. But investors were also less moved by real news once the SEC drew attention to the existence of fake news, demonstrating the potential for fake news to erode the public’s trust in news altogether.
      
      If fake news can disrupt markets, it can affect everyone in society, whether we read and share it or not. More important (as I will show when we discuss the troubling rise of “deepfakes” later in this chapter), if fake news can successfully disrupt markets, it creates a political in- centive for economic terrorism. And as we saw in Crimea, the weap- onization of misinformation is one of the most insidious threats to democracy in the Information Age. The most egregious example to date is Russia’s interference in American democracy in 2016.
      
      The Political Weaponization of Misinformation.
      
      When the Mueller report was released in April 2019, pundits, politi- cians, and the press pored over it, looking for the juiciest bits to sup- port their positions and to entice readers and viewers with salacious headlines. Most of them skipped over volume one and turned directly to volume two, which addressed President Trump's alleged obstruc- tion of the FBI’s Russia investigation. But when I read the Mueller re- port, I was shocked not by the juicy political potential of volume two but by the clear geopolitical reality described in volume one: Russia, an active foreign adversary, had systematically used the Hype Machine to attack American democracy and manipulate the results of the 2016 USS. presidential election. It was one of the most comprehensive weap- onizations of misinformation the world has ever seen.
      
      Two studies commissioned by the U.S. Senate Intelligence Commit- tee, one led by New Knowledge and the other by John Kelly, founder and CEO of Graphika, detail the extent of Russian misinformation cam- paigns targeting hundreds of millions of U.S. citizens in 2016. When I had lunch with John and Graphika’s chief innovation officer, Camille Francois, at the Union Square Café in Manhattan in early 2019, they told me the attack on our democracy was even more sophisticated than the media was giving it credit for. They were both deeply concerned about what they had uncovered. While their report is public, the look on their faces when discussing it was revealing. These two highly regarded experts were worried—and when the experts are worried, we should be too.
      
      Russia’s attack was well plarined. The Internet Research Agency cre- ated fake accounts on Facebook, Twitter, Instagram, YouTube, Google, Tumblr, SoundCloud, Meetup, and other social media sites months, sometimes years, in advance. They amassed a following, coordinated with other accounts, rooted themselves in real online communities, and gained the trust of their followers. Then they created fake news intended to suppress voting and to change our vote choices, in large part toward Republican candidate Donald Trump and away from Democratic candidate Hillary Clinton. The fake news included memes about Black Lives Matter, the mistreatment of American veterans, the Second Amendment and gun control, the supposed rise of sharia law in the United States, and well-known falsehoods like the accusation that Hillary Clinton was running a child sex ring out of the basement of a pizza shop in Washington, D.C. (known as “PizzaGate”). They spread these memes through organic sharing and paid promotion to boost their reach on social media.
      
      On Twitter, they established a smaller number of source accounts, which posted fake content, and close to four thousand sharing ac- counts, which amplified the content through retweets and trending hashtags. The source accounts were manually controlled, while the sharing accounts were more often “cyborg” accounts that were partly automated and partly manual. Automated accounts, run by software robots, or “bots? tweeted and retweeted with greater frequency at pre- specified times. Software doesn't get tired or need bathroom breaks, so the bot army was always on, propagating fake news and engaging the American electorate around the clock.
      
      A lot has been written about the “Great Hack” of 2016. By now we've learned that Russian disinformation was broad based and sophisti- cated. (Footnote: Disinformation is deliberate falsehood spread to deceive, while misinformation is falsehood spread regardless of its intent. Disinformation is a subset of misinforma- tion.). But did it actually change the outcome of the 2016 U.S. presi- dential election (or for that matter the outcome of the Brexit vote in the U.K. or of elections in Brazil, Sweden, and India)? To assess whether it flipped the U.S. election result, we have to answer two additional questions: Was the reach, scope, and targeting of Russian interference sufficient to change the result? And if so, did it successfully change people's voting behavior enough to accomplish that goal?
      
      The Reach, Scope, and Targeting of Russian Election Interference.
      
      During the 2016 election, Russian fake news spread to at least 126 mil- lion people on Facebook and garnered at least 76 million likes, com- ments, and other reactions there. It reached at least 20 million people on Instagram and was even more effective there, amassing at least 187 million likes, comments, and other reactions. Russia sent at least 10 million tweets from accounts that had more than 6 million followers on Twitter. I say “at least” because what we have uncovered so far may just be the tip of the iceberg. Analysis shows, for example, that the twenty most engaging false election stories on Facebook (whether from Russia or elsewhere) in the three months before the election were shared more and received more comments and reactions than the twenty most engaging true election stories. The Hype Machine be- came a clear conduit for misinformation, with one study estimating that 42 percent of visits to fake news websites (and only 10 percent of visits to top news websites) came through social media.
      
      As astonishing as those numbers sound, the scale of fake news in 2016 was considerably smaller than that of real news. For example, in their nationally representative sample of web browsers, Andrew Guess, Brendan Nyhan, and Jason Reifler found that while 44 percent of Americans visited fake news websites in the weeks before the election, these visits comprised only 6 percent of their visits to real news web- sites. Similarly, David Lazer and his colleagues found that only 5 per- cent of registered voters’ total exposures to political URLs on Twitter during the 2016 election were from fake news sources. Hunt Allcott and Matthew Gentzkow estimated the average American saw “one or several” fake news stories in the months before the election.
      
      If these numbers seem small, it’s worth noting some eccentricities in the way these data were collected. News is only considered “fake” in Allcott and Gentzkow’s study if it is one of 156 verified fake news sto- ries. The other two studies analyze restrictive lists of approximately 300 fake news websites. For example, Guess et al. exclude Breitbart,  InfoWars, and all of YouTube when characterizing “fake news sources. Go the 44 percent of voting-age Americans who visited at least one of their restricted list of fake news websites in the final weeks before the election does not count visitors to these popular sources of fake news. In other words, 110 million voting-age Americans visited a narrow list of fake news websites that excluded Breitbart, InfoWars, and YouTube, Our best estimates put the full number of voting-age Americans ex- posed to fake news during the 2016 election at between 110 million and 130 million. So whether voters’ exposure to fake news was “small- scale” is still hotly debated. 
      
      The distributions were skewed by a few viral hits, and a smaller per- centage of voters saw the largest concentration of fake news. Guess et al. found that the 20 percent of Americans with the most conservative news diets were responsible for 62 percent of visits to fake news web- sites and that Americans over sixty were much more likely to consume fake news. On Twitter, Grinberg et al. found that 1 percent of regis- tered voters consumed 80 percent of the fake news and that 0.1 percent accounted for 80 percent of “shares” from fake news websites. In a sec- ond, representative online survey of 3,500 Facebook users, Guess, Nyhan, and Reifler found that only 10 percent of respondents shared fake news and that they were highly concentrated among Americans over sixty-five. This level of concentration, which (as I will describe in Chapter 9) is typical of the Hype Machine, might make you skeptical of the power of fake news to influence a broad swath of society. Butseveral caveats should accompany such skepticism. 
      
      We know that the “superspreaders” and “superconsumers” of fake news, who drive the concentration in these samples, are mostly bots. Grinberg et al. note, for example, that their median supersharer tweeted a whopping seventy-one times a day on average, with an aver-age of 22 percent of their tweets containing fake news URLs, while their median panel member tweeted only 0.1 times a day. The research- ers concluded that “many of these [superspreader and superconsumer] accounts were cyborgs.” Excluding bots, Grinberg et al. found that members of their panel averaged 204 exposures to fake news during the last thirty days of the 2016 campaign, which amounts to seven ex- posures a day. Assuming only 5 percent of these exposures were seen, they estimate that the average human saw one fake news story everythree days leading up to the election.
      
      While some argue fake news isn't important because it constitutes a small part of the average citizens overall media diet, it’s not clear that its volume is proportional to its impact. Fake news is typically sensa- tional and may therefore be more striking and persuasive than every- day news stories. Singular news stories, like Willie Horton, Mike Dukakis riding a tank, and Howard Dean's guttural scream are widely regarded as tipping points in their respective elections. Whether fake news stories behave similarly is an empirical question that remains unanswered. Furthermore, fake news doesn’t live just on social media. It is frequently picked up and repeated by broadcast media and public figures, creating a feedback loop that amplifies its reach beyond social media alone.
      
      The 2000 U.S. presidential election was decided by 537 votes in one key swing state—Florida. Russia's 2016 misinformation campaign tar- geted voters in swing states like Florida, Ohio, Pennsylvania, and Michigan. My colleagues at the Oxford Internet Institute analyzed over 22 million tweets containing political hashtags shared in the week before the election. They geolocated a third of these tweets, tying shar- ers and recipients to the states in which they resided. When they ana- lyzed the geographic distribution of Russian misinformation around the country, they found that “the proportion of misinformation was twice that of the content from experts and the candidates themselves” When they calculated whether a state had more or less Russian fake news, they found that 12 of 16 swing states were above the average. They concluded that Russian fake news was “surprisingly concentrated in swing states, even considering the amount of political conversation occurring in the state” Although more than 135 million votes were cast in the 2016 U.S. presidential election, six swing states (New Hamp- shire, Minnesota, Michigan, Florida, Wisconsin, and Pennsylvania) were decided by margins of less than 2 percent, and 77,744 votes in three swing states (Wisconsin, Michigan, and Pennsylvania) effectively de- cided the election.
      
      On Facebook, Twitter, and Instagram, Russian fake news targeted persuadable voters in swing states with content that was tailored to their interests using “®” mentions and hashtags to draw users to memes custom-designed for them. For example, two days before the election, supporters of the Black Lives Matter movement were drawn to voter suppression memes that encouraged them not to vote. The account @woke_blacks posited a meme to its Instagram account that read, “The excuse that a lost Black vote for Hillary is a Trump win is bs, Should you decide to sit-out the election, well done for the boycott,” while @afrokingdom_ posted that “Black people are smart enough to understand that Hillary doesn’t deserve our votes! DON'T VOTE!” New Knowledge estimated that 96 percent of the Instagram content linked to the Internet Research Agency focused on Black Lives Matter and police brutality, spreading “overt suppression narratives” 
      
      We know that President Trump's former campaign manager, Paul Manafort, shared polling data with the Russian political consultant Konstantin Kilimnik and that targeting persuadable voters in swing states (which is possible with such polling data) was standard operat- ing procedure for Cambridge Analytica, the self-described “election consultancy” that used 87 million Americans’ stolen data to build pre- dictive models of voters’ susceptibility to persuasion, and the topics and content most likely to persuade them. (I will evaluate Cambridge Analytica’s “psychographic profiling” in Chapter 9.) 
      
      If misinformation targeted a small but potentially meaningful number of persuadable voters in key swing states, was the right misin- formation targeted at the right voters? Skeptics contend that such misinformation preached to the choir, because exposure was “selective” meaning that die-hard conservatives saw pro-Trump fake news, while die-hard liberals saw pro-Clinton fake news, making it unlikely that it changed anyone's mind. Guess et al. found 40 percent of Trump sup- porters and only 15 percent of Clinton supporters read pro-Trump fake news, while 11 percent of Clinton supporters and only 3 percent of Trump supporters read pro-Clinton fake news. Sixty-six percent of the most conservative voters, the farthest-right decile, visited at least . one pro-Trump fake news site and read an average of 33.16 pro- Trump fake news articles. 
      
      But the argument that fake news was preaching to the choir doesn't address voter turnout, because ideologically consistent fake news can motivate voters to turn out even if it doesn’t change their vote choices. Furthermore, while voters on the extreme right were disproportion- ately more likely to consume pro-Trump fake news (thus “preaching to the choir”), moderate Clinton supporters and undecideds in the middle of the political spectrum were significantly more likely to consume pro- Trump fake news than pro-Clinton fake news. Could their exposure to
      fake news have persuaded them to vote for Trump or to abstain? That depends on how social media manipulation affects voting.
      
      Social Media Manipulation and Voting.
      
      Was the reach, scope, and targeting of the Russian interference suffi- cient to change the election result? We can’t rule it out. Although expo- sure to fake news was much less than exposure to real news and was concentrated among a select set of voters, it likely reached between 110 million and 130 million people. It didn’t need to affect everyone to tip the election—just a few hundred thousand persuadable voters in key swing states. Which was exactly who Russia targeted. The next big question: did it have an effect on voting? To answer that, we need to understand the science of voter turnout and vote choice. 
      
      Sadly, as I write this, only two published studies link social media exposure to voting. The first, a 61-million-person experiment con- ducted by Facebook during the 2010-US. congressional elections, found social media messages encouraging voting caused hundreds of thousands of additional verified votes to be cast. The second, a follow- up experiment by Facebook during the 2012 presidential election, rep- licated the findings of the first, although the “get out the vote” messages were slightly less effective, as is typical in higher-stakes presidential elections. (I will analyze both of these studies in detail in Chapter 7.) But the main takeaway, for the purposes of our discussion of the 2016 election, is that social media messaging can significantly increase voter turnout, with minimal effort. While there are only two large-scale studies of the effect of social media on voting, the substantial research on the effects of persuasive messaging on voter turnout and vote choice can help us to calibrate the likely effects of Russian interference on the 2016 election.” (Fotenote: Chris Bail and his team found no evidence that interactions with IRA Twitter ac- counts in late 2017 impacted political attitudes or behavior. But several limitations prevented them from determining “whether IRA accounts influenced the 2016 presi- dential election” The study was conducted a year after the election, after the IRA had ramped down their information operation and Twitter had suspended two-thirds of their accounts. The sample included no independents and only frequent Twitter users, was not representative of U.S. voters, and did not consider voting behavior. The study. did find suggestive evidence that IRA interactions changed three things: opposing party ratings among “low news interest” respondents, the number of political accounts followed among “high news interest” respondents, and both opposing party ratings and the number of political accounts followed among Democrats.)
      
      With regard to vote choice, some meta-analytic reviews suggest the effects of impersonal contact (mailings, TV, and digital advertising) on vote choice in general elections are very small. Kalla and Broockman conclude, from a meta-analysis of forty-nine field experiments, that “the best estimate of the size of persuasive effects [i.e., effects of adver- tising on vote choice] in general elections . . . is zero” But their data do not consider social media. And there is substantial uncertainty in their estimates, such as the effect of impersonal contact within two months of Election Day, which was when Russias attack was in full swing.
      
      They also found significant effects of persuasive messages on vote choice in primaries, on issue-specific ballot measures, and on cam- paign targeting of persuadable voters. Rogers and Nickerson found, for example, that informing voters in favor of abortion rights that a candidate did not support such rights had a 3.9 percent effect on re- ported vote choice, suggesting the possibility that targeted, issue- specific manipulation of the type Russia deployed could have changed vote choices. The power of persuasive messaging on issue-specific ballot measures also highlights the possibility of interference being directed at large numbers of regional elections and local policy measures, which could collectively shift the political direction of a country without hav- ing to impact results at a national level. This threat is particularly in- sidious, because it would be much harder to detect than interference in a general election.
      
      Moreover, social media manipulation does not have to change our vote choices to tip an election. Targeted efforts to increase or diminish voter turnout could be substantial enough to change overall election results, and recent evidence suggests targeted messaging can affect voter turnout. For example, a randomized experiment by Katherine Haenschen and Jay Jennings showed that targeted digital advertis- ing significantly increased voter turnout among millennial voters in competitive districts. Research by Andrew Guess, Dominique Lock- ett, Benjamin Lyons, Jacob Montgomery, Brendan Nyhan, and Jason Reifler showed that randomized exposure to just a single misleading article increased belief in the article's claims and increased self-reported intentions to vote. The meta-analysis by Green et al. estimated that direct mailings, combined with social pressure, generate an average increase in voter turnout of 2.9 percent, while canvassing generates an average increase of 2.5 percent, and volunteer phone banks generate an average increase of 2 percent. Dale and Strauss estimated the effect of text messages on voter turnout to be 4.1 percent, and there is evi- dence that personalized emails have a similar impact. The only studies of the impact of voter turnout as a result of social media messaging estimate that hundreds of thousands of additional votes were cast as a result of such messages. 
       
      So did Russian interference flip the 2016 U.S. presidential election or not? A full presidential term after the first Russian intervention and on the eve of the 2020 presidential election, during which Russia andothers are continuing to interfere, we still don’t know. What's scary is we cart rule it out. Its certainly possible given all the evidence. We don’t know because no one has studied it directly. Unfortunately, until we do democracies worldwide will remain vulnerable.
      
      The 2020 U.S. Presidential Election.
      
      We should all have been alarmed when Special Counsel Robert Muel- ler testified that “the Russian government's effort to interfere in our election is among the most serious . . . challenges to our democracy” he has ever seen. He stressed that the threat “deserves the attention of every American” because the Russians “are doing it as we sit here and they expect to do it during the next campaign” He concluded that “much more needs to be done to protect against these intrusions, notjust by the Russians but others.” 
       
      FBI director Christopher Wray is warning that “the threat just keeps: escalating” Not only have Russia's attacks intensified in 2020, but other countries, like China and Iran, are “entertaining whether to take a page out of that book” It is not clear what will happen to Americans trust in democracy if the 2020 election Is a more convincing, more deter- mined rerun of 2016. 
       
      What is clear is that the 2020 election is being targeted. In February:intelligence officials informed the U.S. House Intelligence Committee that Russia was intervening to support President Trump's reelection. In March, the FBI informed Bernie Sanders that Russia was trying to tip the scales on his behalf. Intelligence officials have warned that Rus- sia has adjusted their playbook toward newer, less easily detectable tactics to manipulate the 2020 election. Rather than impersonating Americans, they are now nudging American citizens to repeat mis- information to avoid social media platform rules against “inauthen- tic speech” They have shifted their servers from Russia to the United States because American intelligence agencies are barred from domes- tic surveillance. They have infiltrated Iran's cyberwar department, per- haps to launch attacks made to seem like they originated in Tehran. In November 2019, Russian hackers successfully infiltrated the servers of the Ukrainian gas company Burisma, which was at the center of widely discredited allegations against Joe Biden and his son Hunter Biden, perhaps in an effort to dig up dirt to use during the general election. would not be at all surprised to see a rerun of the Hillary Clinton email scandal emerge as a Joe Biden Burisma scandal in the fall of 2020. It wouldn't be hard for foreign adversaries to seed false material into the American social media ecosystem, made to seem like real material from the successful Burisma hack, to create a scandal designed to de- rail the Biden campaign before anyone can debunk it. As we've seen, this is the signature of a fake news crisis: it spreads faster than it can be corrected, so it’s hard to clean up, even with a healthy dose of the truth.
      
      The threat of election manipulation in 2020 is even higher due to the chaos caused by the coronavirus pandemic. With uncertainty around the viability of in-person voting, questions about voting by mail, and calls to delay the election, there can be no doubt that foreign ac- tors will look to leverage the confusion caused by the coronavirus to disrupt our democratic process. Extreme vigilance is warranted dur- ing these difficult times when our vulnerability to manipulation is par- ticularly high.
      
      While some claim fake news is benign, during protests and confu- sion, amid the smoke, fire, and foreign interference, months from the most consequential election of our time, it is a real threat—not only to the election, but to the sanctity and peace of the election process. If the election were to be contested, fake news could escalate the contest, per- “haps to violence. As Donald Horowitz has noted, “Concealed threats ‘and outrages committed in secret figure prominently in pre-riot rumors. Since verification of such acts is difficult, they form the perfect content for such rumors, but difficulty of verification is not the only way in which they facilitate violence. . .. Rumors form an essential part of the riot process. They justify the violence that is about to occur. Their severity is often an indicator of the severity of the impending violence. Rumors narrow the options that seem available to those who join crowds and commit them to 2 tine of action. They mobilize ordi- nary people to do what they would not ordinarily do. They shift the balance in a crowd toward those proposing the most extreme action, They project onto the future victims of violence the very impulses en- tertained by those who will victimize therm. They confirm the strength and danger presented by the target group, thus facilitating violence born of fear” Extreme vigilance is warranted during these difficult times when our vulnerability to manipulation is particularly high. 
      
      The threat of misinformation is not limited to Russia or American democracy. Digital interference threatens democracies worldwide. Car- ole Cadwalladr’s investigative reporting on the role of fake news in the Rrexit vote and her work with Christopher Wiley to break the Cam- bridge Analytica scandal for The Guardian gave us a glimpse into the extent to which fake news has been weaponized around the world. Re: search by the Oxford Internet Institute found that one out of everythree URLs with political hashtags being shared on Twitter ahead © the 2018 Swedish general election were from fake news sources.
      
      A study of the federal University of Mina Gerais, the University of Sao Paulo, and the fact-checking platform Agencia Lupa, which ana: were misleading, and only 8 percent were fully truthful. Microsoft es timated that 64 percent of Indians encountered fake news online ahea of Indian elections in 2019. In India, where 52 percent of people repo getting news from WhatsApp, private messaging is a particularly sidious breeding ground for fake news, because people use priva counteract the spread of falsity. In the Philippines, the spread of mi information propagated to discredit Maria Ressa, the Filipino-Ameri journalist working to expose corruption and a Time Person of the Year in 2018, was vast and swift. Similar to the Russian influence operation in Crimea, the misinformation campaign against Ressa mirrored the charges that were eventually brought against her in court. In June 2020, she was convicted of cyber libel and faced up to seven years in prison. The weaponization of misinformation and the spread of fake news are problems for democracies worldwide.
      
      
      Fake News as Public Health Crisis. In March 2020, a deliberate misinformation campaign spread fear among the American public by propagating the false story that a na- tionwide quarantine to contain the coronavirus pandemic was immi- nent. The National Security Council had to publicly disavow the story. And that wasn't the only fake news spreading about the virus. The Chi- nese government spread false conspiracy theories blaming the US. military for starting the pandemic. Several false coronavirus “cures” killed hundreds of people who drank chlorine or excessive alcohol to rid themselves of the virus. There was, of course, no cure or vaccine at the time. International groups, like the World Health Organization (WHO), fought coronavirus misinformation on the Hype Machine as _ part of their global pandemic response. My group at MIT supported the COVIDConnect fact-checking apparatus, the official WhatsApp coronavirus channel of the WHO, and studied the spread and impact of coronavirus misinformation worldwide. 
      
      
      
      But we first glimpsed the destructive power of health misinformation on the Hype Machine the year before the coronavirus pandemic hit, during the measles resur- gence of 2019. -Measles was declared eliminated in the United States in 2000. But while only 63 cases were reported in 2010, over 1,100 cases were re- ported in the first seven months of 2019, a nearly 1,800 percent in- crease. Measles is particularly dangerous for kids. It typically starts as a fever and rash, but in one in a thousand cases, it spreads to the brain causing cranial swelling and convulsions, or encephalitis. In one in enty kids, it causes pneumonia, preventing the lungs from extract- g oxygen from the air and delivering it to the body. The disease took ie lives of 110,000 kids worldwide this way in 2017 
      
      Measles is one of the world’s most contagious viruses. You can catch it from droplets of air contaminated with an infected persons cough hours after they've left the room. Nine of ten people exposed to it con- tract it. While the average number of people infected by one person with the coronavirus of 2020—its RO figure—was 2.5, the RO of mea- sles is 15.
      
      To prevent such a contagious disease from spreading, society has to develop herd immunity by vaccinating a large percentage of the popu- lation. With polio, which is less contagious, herd immunity can be achieved with 80 to 85 percent vaccination rates. For a highly contagious virus like measles, 95 percent of the population should be vaccinated to achieve herd immunity. Sadly, while there has been an effective vac- cine since 1963, the resurgence of measles in the United States was driven by vaccine refusal, according to experts. While 91 percent of young children got the measles-mumps-rubella (or MMR) vaccine in 2017, vaccination rates in some communities fell dramatically in re- cent years, and it is in exactly these communities that measles skyrock- eted.
      
      This outbreak hits close to home for me because I have a six-year- old son, and the hardest-hit group, representing over half of all reported measles cases in the United States in 2019, was the Orthodox Jewish community located five blocks from our home in Brooklyn, New York. Other countrywide outbreaks have been clustered in: tight-knit communities like the Jewish community in Rockland County, New York, and the Ukrainian- and Russian-American com+ munities of Clark County, Washington, where vaccination rates: hover around 70 percent, well below the threshold needed for herd : immunity. 
      
      If measles is so dangerous and vaccines are so effective, why are some parents not vaccinating their kids? The answer lies partly in a wave of misinformation about the dangers of vaccination that began in Wakefield in the respected medical journal The Lancet, that claimed to link vaccines to autism. It was later revealed that Wakefield had been paid by lawyers 1998 with a discredited paper, published by Andrew suing the vaccine manufacturers, and that he was developing a com peting vaccine himself. The Lancet promptly retracted the paper, and Wakefield lost his medical license. But the wave of misinformation he created continues today, aided by conspiracy theories propagated on blogs, in a widely circulated movie called Vaxxed directed by Wake- field himself, and most recently on social media.
      
      In March 2019, addressing ‘this wave of antivax misinformation dur- ing a public US. Senate hearing, Dr. Jonathan McCullers, chair of the department of pediatrics at the University of Tennessee Health Science Center and pediatrician-in-chief at Le Bonheur Children’s Hospital in - Memphis, testified that, in addition to state policies on vaccine exemp- tions and methods of counseling, “social media and the amplification of minor theories through rapid and diffuse channels of communica- tion, coupled with instant reinforcement in the absence of authorita- tive opinions, is now driving .. . vaccine hesitancy. When parents get “much of their information from the Internet or social media platforms such as Twitter and Facebook, reading these fringe ideas in the ab- sence of accurate information can lead to understandable concern and confusion. These parerits may thus be hesitant to get their children “vaccinated without more information” The anecdotal evidence that social media misinformation drives the spread of vaccine-preventable diseases like measles is troubling.
      
      The Antivax King of Facebook. Larry Cook describes himself as a “full time [antivax] activist” As of 2019, he was also the antivax king of Facebook. His Stop Mandatory Vaccination organization is a for-profit entity that makes money ped- dling antivax fake news on social media and earning referral fees from ales of antivax books on Amazon. He also raises money through soFundMe campaigns that pay for his website, his Facebook ads, and ‘his personal bills. Cooks Stop Mandatory Vaccination and another or- anization called the World Mercury Project, headed by Robert E Ken- edy, Jr., bought 54 percent of the antivaccine ads on Facebook in 019.
      
      Cooks antivax Facebook ad campaigns targeted women over enty-five in Washington State, a population likely to have kids who eed vaccines. More than 150 such posts aimed at such women, pro- moted by seven Facebook accounts including Cooks, were viewed 1.6 nillion to 5.2 million times and garnered about 18 clicks per dollar pent on the campaigns. Facebook's average cost per click across all industries hovers around $1.85. When you do the math, you see that Cook's reach is insanely efficient. These data suggest he pays about six cents per click. 
      
      In early 2019, Facebook search results for information about vac- cines were dominated by antivaccination propaganda. YouTube's rec- ommendation algorithm pushed viewers “from fact-based medical information toward anti-vaccine misinformation, and “75% of Pin- terest posts related to vaccines discussed the false link between mea- sles vaccines and autism” In a paper published in 2019, researchers at George Washington University found that Russian Twitter bots were posting about vaccines twenty-two times more often than the average Twitter user, linking vaccination misinformation to Russia's efforts to hijack the Hype Machine. 
      
      Like political fake news, antivax misinformation is concentrated. Analysis by Alexis Madrigal of The Atlantic showed that the top 50 vaccine pages on Facebook accounted for nearly half of the top 10,000 vaccine-related posts and 38 percent of all the likes on these posts from January 2016 to February 2019. In fact, just seven antivax pages generated 20 percent of the top 10,000 vaccine posts during this time. 
      
      As I will describe in the next chapter, the Hype Machines networks are highly clustered around tight-knit communities of people withsimilar views and beliefs. We live in an information ecosystem that connects like-minded people. The measles outbreaks in New York and Washington in 2019 and 2020 are taking place in tight-knit communi- ties of like-minded people. In the same way that Russian misinforma- tion wouldn't need to convince the majority of Americans to affect elections (small numbers in key swing states would be enough), anti- vax social media campaigns wouldn't have to convince large swaths of people to forgo vaccinations to create an outbreak. To bring the levels of vaccination below the thresholds needed for herd immunity, they would only have to convince small numbers of people in tight-knit communities, who would then share the misinformation among them: selves. 300,000 vaccine-related posts over seven years found that the vaccin consumption of content about vaccines is dominated by the ech 
      
      Research that analyzed the interactions of 2.6 million users with 300,000 vacine-related posts over seven years found that the vaccine conversations taking place on Facebook were happening in exactly these types of tight-knit communities. ‘The findings showed that the consumption of content about vaccine is dominated by the echo chamber effect and that polarization increased over the years. Well separated communities emerged from the users’ consumption hab- its. . . . The majority of users consume in favor or against vaccines, not both” These tight-knit communities in Washington State are the exact communities that Larry Cook and antivax profiteers have targeted with their Facebook ads. They are also the same communities in which disease outbreaks are occurring.
      
      In early 2019, social media platforms took notice. Instagram began blocking antivaccine-related hashtags like #vaccinescauseautism and #vaccinesarepoison. YouTube announced it is no longer allowing users to monetize antivaccine videos with ads. Pinterest banned searches for vaccine content. Facebook stopped showing pages and groups featur- ing antivaccine content and tweaked its recommendation engines to stop suggesting users join these groups. They also took down the Face- book ads that Larry Cook and others had been buying. The social plat- forms took similar steps to stern the spread of coronavirus fake news in 2020. Will these measures help slow the coronavirus, measles out- breaks, and future pandemics? Will fake news drive the spread of pre- ventable diseases? Answers to these questions lie in the emerging science of fake news,
      
      The Science of Fake News.
      
      Despite the potentially catastrophic consequences of the rise of fake : mews for our democracies, our economies, and our public health, the science of how and why it spreads online is still in its infancy. Until 2018, most scientific studies of fake news had only analyzed small samples or case studies of the diffusion of single stories, one at a time My colleagues Soroush Vosoughi and Deb Roy and 1 set out to chan c that when we published our decade-long study of the spread of ale news online in Science in March 2018. 
      
      In that study, we collaborated directly with Twitter to study the dif- fusion of all the verified true and false rumors that had ever spread on the platform from its inception in 2006 to 2017. We extracted tweets about verified fake news from Twitter's historical archive. The data in- cluded approximately 126,000 Twitter cascades of stories spread b 3 million people over 4.5 million times. We classified news as true or false using information from six independe ing and others) that exhib- tions (including Snopes, PolitiFact, FactCheck, ited 95 to 98 percent agreement on the veracity of the news. We then employed students working independently at MIT and Wellesley Co - ck for bias in how the fact-checkers had chosen those stories. 
      
      Once we had a comprehensive database of verified rumors span- of Twitter's existence, we searched Twitter for men- followed their sharing activity to the “origin” on of a story on Twitter), and re-created the 11 chains of retweets with a common, singu- lar origin) of these stories spreading online. When we visualized these cascades, the sharing activity took on bizarre, alien-looking shapes: They typically began with a starburst pattern of retweets emanating from the origin tweet and then spread out, with new retweet chains that looked like the tendrils of a jellyfish trailing away from the star- burst. I've included an image of one of these false news cascades in Figure 2.2. These cascades can be characterize lege to che   Once we h ning the ten years tions of these stories, tweets (the first menti retweet cascades (unbroke of a false news story. through Twitter. Lon ger lines represent lo ger retweet cascades, demonstrating the greater breadth and depth at which false news spreads. d mathematically as they spread across the Twitter population over time. So we analyzed how the false ones spread differently than the true ones. 
       
      Our findings surprised and disturbed us. In all categories of infor- mation, we discovered, false news spread significantly farther, faster, deeper, and more broadly than the truth—sometimes by an order of magnitude. While the truth rarely diffused to more than 1,600 people, the top 1 percent of false news cascades routinely diffused to as many as 100,000 people. It took the truth approximately six times as Jong as falsehood to reach 1,500 people and twenty times as long to travel ten reshares from the origin tweet in a retweet cascade. Falsehood spread significantly more broadly and was retweeted by more unique users than the truth at every cascade depth. (Each reshare spreads the infor- mation away from the origin tweet to create a chain or cascade of re- shares. The number of links in a chain is the cascade’s “depth’”).
      
      False political news traveled deeper and more broadly, reached more people, and was more viral than any other category of false news. It ‘reached more than 20,000 people nearly three times faster than all other types of false news reached just 10,000 people. News about poli- tics and urban legends spread the fastest and was the most viral. False- hoods were 70 percent more likely to be retweeted than the truth, even when controlling for the age of the account holder, activity level, and umber of followers and followees of the original tweeter and whether the original tweeter was a verified user.
      
      While one might think that characteristics of the people spreading he news would explain why falsity travels with greater velocity than he truth, the data revealed the opposite. For example, one might sus- pect that those who spread falsity had more followers, followed more eople, tweeted more often, were more often “verified” users, or had een on Twitter longer. But the opposite was true. People who spread alse news had significantly fewer followers, followed significantly ewer people, were significantly less active on Twitter, were “verified” ignificantly less often, and had been on Twitter for significantly less Ime, on average. In other words, falsehood diffused farther, faster, deeper, and more broadly than the truth despite these differences, not because of them. So why and how does fake news spread?
      
      Lies spread online through a complex interaction of coordinated bots and unwitting humans working together in an unexpected symbiosis.
      
      Social Bots and the Spread of Fake News Social bots (software-controlled social media profiles) are a big part of how fake news spreads. We saw this in our Twitter data when we ana- lyzed Russia's information operation in Crimea in 2014 and in the de- cade of data in our broader Twitter sample. The way social bots are used to spread lies online is both disturbing and fascinating. 
       
      In 2018 my friend and colleague Filippo Menczer at Indiana Uni- versity, along with his colleagues Chengcheng Shao, Giovanni Ciam- paglia, Onur Varol, Kai-Cheng Yang, and Alessandro Flammini, published the largest-ever study on how social bots spread fake news. They analyzed 14 million tweets spreading 400,000 articles on Twitter in 2016 and 2017. Their work corroborated our finding that fake news was more viral than real news. They also found that bots played a big role in spreading content from low-credibility sources. But the way bots worked to amplify fake news was surprising, and it highlights the sophistication with which they are programmed to prey on the HypeMachine.
      
      First, bots pounce on fake news in the first few seconds after it’s published, and they retweet it broadly. That's how they're designed. And the initial spreaders of a fake news article are much more likely to be bots than humans. Think about the starburst pattern in the Twitter cascade of fake news shown in Figure 2.2. Many of these starbursts are created by bots. What happens next validates the effectiveness of this strategy, because humans do most of the retweeting. The early tweeting activity by bots triggers a disproportionate amount of human engage- ment, creating cascades of fake news triggered by bots but propagated by humans through the Hype Machine's network.
      
      Second, bots mention influencial humans incessantly. If they can get an influential human to retweet fake news, it simultaneously amplifies and legitimizes it. Menczer and his colleagues point to an example in their data in which a single bot mentioned @realDonald Trump nine- teen times, linking to the false news claim that millions of votes were. cast by illegal immigrants in the 2016 presidential election. The strat- egy works when influential people are fooled into sharing the content Donald Trump, for example, has on a number of occasions shared content from known bots, legitimizing their content and spreding their misinformation widely in the Twitter network. It was Trump who adopted the false claim that millions of illegal immigrants voted in the 2016 presidential election as an official talking point.
      
      But bots can't spread fake news without people. In our ten-year study with Twitter, we found that it was humans, more than bots, that helped make false rumors spread faster and more broadly than the truth. In their study from 2016 to 2017, Menczer and his colleagues also found that humans, not bots, were the most critical spreaders of fake news in the Twitter network. In the end, humans and machines play symbiotic roles in the spread of falsity: bots manipulate humans to share fake news, and humans spread it on through the Hype Ma- chine. Misleading humans is the ultimate goal of any misinformation campaign. It's humans who vote, protest, boycott products, and decide whether to vaccinate their kids. These deeply human decisions are the very object of fake news manipulation. Bots are just a vehicle to achieve an end. But if humans are the objects of fake news campaigns, and if they are so critical to their spread, why are we so attracted to fake news? And why do we share it?
      
      
      The Novelty Hypothesis.
      
      One explanation is what Soroush Vosoughi, Deb Roy, and 1 called the novelty hypothesis. Novelty attracts human attention because it is sur- prising and emotionally arousing. It updates our understanding of the world. Tt encourages sharing because it confers social status on the sharer, who is seen as someone who is “in the know” or who has access to “inside information” Knowing that, we tested whether false news was more novel than the truth in the ten vears of Twitter data we stud- an influential human to retweet fake news, it simultaneously amplifies and legitimizes it. Menczer and his colleagues point to an example in their data in which a single bot mentioned @realDonald Trump nine- teen times, linking to the false news claim that millions of votes were. cast by illegal immigrants in the 2016 presidential election. The strat- egy works when influential people are fooled into sharing the content Donald Trump, for example, has on a number of occasions shared content from known bots, legitimizing their content and spreadin ied. We also examined whether Twitter users were more likely to retweet information that seemed to be more novel, 
      
      To assess novelty, we looked at users who shared true and false ru- mors and compared the content of rumor tweets to the content of all the tweets the users were exposed to in the sixty days prior to their decision fo retweet a rumor. Our findings were consistent across mul- tiple measures of novelty: false news was indeed more novel than the truth, and people were more likely to share novel information. This makes sense in the context of the “attention economy” (which I will discuss in detail in Chapter 9). In the context of competing social media memes, novelty attracts our scarce attention and motivates our consumption and sharing behaviors online. 
      
      Although false Tumors were more novel than true rumors in our study, users may not have perceived them as such. So to further test our novelty hypothesis, we assessed users’ perceptions of true and false rumors by comparing the emotions they expressed in their replies to se rumors. We found that false rumors inspired more surprise and disgust, corroborating the novelty hypothesis, while the truth inspired more sadness, anticipation, joy, and trust. These emotions shed light on what inspires people to share false news beyond its novelty. To un- derstand the mechanisms underlying the spread of fake news, we have to also consider humans’ susceptibility to it.
      
      Our Susceptibility to Fake News The science of human susceptibility to vated reasoning’ Classical reasoning contends analytically, tivated reasoning, minded liefs, especia beliefs to begin with. ognize fake news. They m were using a cognitive reflecti simple puzzle, like this one: costs $1.00 more than the ball. How muchproblem elicits a fast, intuitive response—ten cents—t false beliefs is more developed than the science of fake news but is, unfortunately, no more settled. There's currently a debate between “classical reasoning” and “moti- that when we think we are better able to tell whats real from what's fake. Mo- on the other hand, contends that when we are faced with corrective information about a false belief, the more analytically of us “dig in” and increase our commitment to those false be- lly if we are more partisan or committed to those false. 
      
      My friend and colleague David Rand at MIT teamed up with Gor-. don Pennycook to study what types of people were better able to rec casured how cognitively reflective peopleon task (CRT) and then asked them whether they believed a series of true and false news stories. A cogil tive reflection task tests how reflective someone is by giving them a “A bat and ball cost $1.10 in total. The bat does the ball cost?” The hat, upon reflection, is wrong: if the ball cost ten cents, the bat would have to cost $1.10 and they would total $1.20, Asking people to consider these types of puzzles tests their reflectiveriess. And Rand and Pennycook found that people who were more reflective were better able to tell truth from falsity and to recognize overly partisan coverage of true events, sup- porting classical reasoning.
      
      But repetition causes belief. If you beat us over the head with fake news, were more likely to believe it. It’s called the “illusory truth effect”—we tend to believe false information more after repeated ex- posure to it. People also tend to believe what they already think. (Thats confirmation bias.) So the more we hear something and the more it aligns with what we know, the more likely we are to believe it. Similar thinking has led some cognitive and political scientists to h rpoth- esize that because of confirmation bias, corrective information can backfire—that trying to convince someone that their falsely held be- lief is wrong actually causes them to dig in to those false beliefs even more. But so far, evidence for this “backfire effect” seems weaker. For example, in three survey experiments, Andrew Guess and Alexander €oppock found “no evidence of backlash, even under theoretically fa- vorable conditions.”
      
      So reflection helps us distinguish truth from falsity, repetition causes belief, and corrective information doesn’t seem to backfire even though a confirmation bias generally leads us to believe what we al- ready know. These findings give us leads for fighting fake news (which Lwill return to in Chapter 12, when | discuss how we must adapt). 
      
      The Economic Mative to Create Fake News. 
      
      The political motive for creating fake news is abundantly clear from ; asias foreign interference in Ukrainian and American politics. But » economic motive should not be underestimated. And nowhere has the economic motive to create fake n f i ews been more obvious than in Veles, Macedonia. 
      
      Veles is a sleepy mountain town with 55,000 residents, two TV nels, and a few lovely churches. Tt boasts a handful of notable sistorical figures and events, from Ottoman grand viziers to battles
      between the Serbian and Ottoman empires in the late fourteenth cen- tury. But perhaps the most important contribution to Veless global historical significance will be that during the 2016 U.S. presidential election, its unemployed teen population discovered how the Hype Machine could make them rich by spreading fake news online. 
      
      The teenagers of Veles developed and promoted hundreds of web- sites that spread fake news to voters in the United States through social media advertising networks. Companies like Google show ads to In- ternet browsers and pay website creators based on how many high- quality eyeballs they attract. The teenagers of Veles discovered that they could make a lot of money by creating websites and promoting their content though social media networks. The more people read and shared their articles, the more money they made. 
      
      They found that fake news attracted more readers and, as we found in our own research, that it was 70 percent more likely to be shared online. They created fake accounts to amplify the signal, and once the trending algorithms got hold of them, the fake news stories received a broadcasting boost, exposing them to even more people, in new areas of the network. What ensued was a deluge of fake news that washed over the American public just as they were heading to the polls. Money flowed in one direction and falsehood flowed in the other, leaving Veles flush with new BMWs, and the United States inundated with false news months before the 2016 presidential election. The town of Veles is only one such example. In 2019 fake news websites generated. over $200 million a year in ad revenue. Fake news is big business, andour approaches to solving the problem (which [ will address in Chap- ; ter 12) must recognize that economic reality.
      
      The End of Reality.
      
      Unfortunately, everything I have described so far—from stock market crashes to coronavirus misinformation to measles outbreaks to elec tion interference—is the good news. That's because the age of fake news is about to get a whole lot worse. We are on the verge of a new eraof synthetic media that some fear will usher us into an “end of reality This characterization may seem dramatic, but there is no doubt that  technological innovation in the fabrication of falsity is advancing at a breakneck pace. The development of “deepfakes” is generating exceed- ingly convincing synthetic audio and video that is even more likely to fool us than textual fake news. Deepfake technology uses deep learn- ing, a form of machine learning based on multilayered neural net- works, to create hyperrealistic fake video and audio. If seeing is believing, then the next generation of falsity threatens to convince us more than any fake media we have seen so far.
      
      In 2018 movie director (and expert impersonator) Jordan Peele teamed up with BuzzFeed to create a deepfake video of Barack Obama calling Donald Trump a “complete and total dipshit.” It was convincing but obviously fake. Peele added a tongue-in-cheek nod to the obvious falsity of his deepfake when he made Obama say, “Now, I would never say these things . . . at least not in a public address” But what happens when the videos are not made to be obviously fake, but instead made to convincingly deceive?
      
      
      Deepfake technology is based on a specific type of deep learning called generative adversarial networks, or GANS, which was first de- veloped by lan Goodfellow while he was a graduate student at the Uni- versity of Montreal. One night while drinking beer with fellow graduate students at a local watering hole, Goodfellow was confronted with a machine-learning problem that had confounded his friends: training a computer to create photos by itself. Conventional methods were fail- ing miserably. But that night while enjoying a few pints, Goodfellow had an epiphany. He wondered if they could solve the problem by pit- ting two neural networks against each other. It was the origin of GANs—a technology that Yann LeCun, former head of Facebook Al Research, dubbed “the coolest idea in deep learning in the last 20 years.” It's also what manipulated Barack Obama to call Donald Trump a “dipshit”
      
      GANS pit two networks against each other: a “generator,” whose job is to generate synthetic media, and a “discriminator whose job is to determine if the content is real or fake. The generator learns from the discriminator’ decisions and optimizes its media to create more and more convincing video and audio. In fact, the generators whole job is to maximize the likelihood that it will fool the discriminator into thinking the synthetic video or audio is real. Imagine a machine, set in a hyper loop, trying to get better and better at fooling us. Thats the future of reality distortion in a world with exponentially improving GANs technology. 
      
      GANS can be used for good as well—for example, to generate con- vincing synthetic data in high-energy physics experiments or to ac- celerate drug discovery. But the potential geopolitical and economic harm they can create is troubling. Ambassador Daniel Benjamin, for- mer coordinator for counterterrorism at the US. State Department, and Steven Simon, former National Security Council senior director for counterterrorism in the Clinton and Obama administrations, paint a grim picture: “One can easily imagine the havoc caused by falsified video that depicts foreign Iranian officials collaborating with terrorists to target the United States. Or by something as simple as invented news reports about Iranian or North Korean military plans for pre- emptive strikes on any number of targets... It might end up causing a war, or just as consequentially, impeding a national response to a genuine threat”.
      
      Deepfaked audio is already being used to defraud companies of millions of dollars. In the summer of 2019, Symantec CTO Hugh Thompson revealed that his company had seen deepfaked andio at- tacks against several of its clients. The attackers first trained a GAN on hours of public audio recordings of a CEO's voice, while giving news interviews, delivering public speeches, speaking during earnings calls, or testifying before Congress. Using these audio files, the attack ers built a system to automatically mimic the CEO's voice. They would call, for example, the CFO of the company and pretend to be the CEQ requesting an immediate wire transfer of millions of dollars into bank account they controlled. The system didn’t just deliver a prere corded message but converted the attacker's voice into the CEO's voi in real time so they could engage in a realistic conversation and answe the CFO's questions. The synthesized audio of the CEO's voice was's convincing that, coupled with a good story about why the monneeded to be transferred right away—they were about to lose a bi deal, or they had to beat an impending deadline at the end of the fist quarter—the CFO would comply with the CEO's request and execs the transfer. Thompson revealed that each attack cost the target co panies millions of dollars. 
      
      As Jordan Peele made Obama say in his BuzzFeed deeptake: "It may sound basic, but how we move forward, in the Age of Information, is going to be the difference between whether we survive or whether we become some kind of fuckéd up dystopia". To understand whether dystopia is our destiny, we have to understand how the Hype Machine works. To do that, we'll need to go back to first principles, starting with a deep dive under the hood of the Hype Machine, followed by an ex- amination of social media's effect on our brains.
      
      
      
      </pre>


  </main>
  <footer>
    <address><a href="imprint.html">Imprint</a> | <a href="privacy.html">Privacy</a> | <a
        href="https://github.com/afdzlebron/22ws_bok_html_project" target="_blank">Github-Repository</a>
    </address>
  </footer>
  <script src="https://code.jquery.com/jquery-latest.js"></script>
  <script src="http://cdn.rawgit.com/noelboss/featherlight/1.7.1/release/featherlight.min.js" type="text/javascript"
    charset="utf-8"></script>

  <script type="text/javascript">
    $(function () {
      $('header a.icon').click(function () {
        $('nav').toggleClass('responsive');
      });
    });
  </script>
</body>

</html>